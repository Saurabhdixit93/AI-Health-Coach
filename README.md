# Disha AI Health Coach

India's first AI health coach - A mini AI health coach chat application with WhatsApp-like UX, featuring intelligent context management, long-term memory, and medical protocol integration.

![Disha AI Health Coach](https://img.shields.io/badge/Status-Production%20Ready-green) ![Python](https://img.shields.io/badge/Python-3.9+-blue) ![Next.js](https://img.shields.io/badge/Next.js-14-black) ![FastAPI](https://img.shields.io/badge/FastAPI-0.109-teal) ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Latest-blue)

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Architecture Overview](#-architecture-overview)
- [Local Setup](#-local-setup)
- [Deployment Guide](#-deployment-guide) ğŸš€
- [How It Works](#-how-it-works)
- [API Documentation](#-api-documentation)
- [LLM Integration](#-llm-integration)
- [Trade-offs & Design Decisions](#-trade-offs--design-decisions)
- [Future Improvements](#-future-improvements)

---

## âœ¨ Features

### Core Features

- **WhatsApp-like Chat Interface**: Familiar, conversational UI that users already know
- **Infinite Scroll**: Load chat history automatically as you scroll up
- **Smart Context Management**: Prevents token overflow while maintaining conversation quality
- **Long-term Memory**: AI remembers important user details across conversations
- **Medical Protocol Matching**: Automatically applies relevant health protocols
- **Real-time Typing Indicators**: Know when AI is generating a response
- **Onboarding Flow**: Intelligent conversation to gather user context

### Technical Features

- **Cursor-based Pagination**: Efficient message history loading
- **Redis Caching**: Fast access to protocols and typing indicators
- **Token Budget Management**: Keeps LLM calls within limits
- **Robust Error Handling**: Graceful degradation and user-friendly error messages
- **Auto-scroll**: Automatically scrolls to latest message
- **Input Validation**: Prevents empty messages, handles edge cases

---

## ğŸ›  Tech Stack

### Backend

- **Framework**: FastAPI (Python) - High performance, async support, auto-generated docs
- **Database**: PostgreSQL - Reliable relational database with JSONB support
- **Cache**: Redis - Fast in-memory data store for sessions and frequently accessed data
- **ORM**: SQLAlchemy - Powerful Python SQL toolkit and ORM
- **LLM**: OpenAI via OpenRouter - Cost-effective AI inference

### Frontend

- **Framework**: Next.js 14 (TypeScript) - React framework with app router
- **Styling**: TailwindCSS - Utility-first CSS framework
- **HTTP Client**: Axios - Promise-based HTTP client
- **Date Formatting**: date-fns - Modern JavaScript date utility library

### Infrastructure

- **Database Migrations**: Alembic (optional, using SQLAlchemy create_all for simplicity)
- **Environment Management**: python-dotenv, Pydantic Settings
- **Type Safety**: TypeScript (Frontend), Pydantic (Backend)

---

## ğŸ— Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend                            â”‚
â”‚                      (Next.js + React)                      â”‚
â”‚                                                             |
â”‚  â€¢ ChatInterface: Main chat UI with infinite scroll         â”‚
â”‚  â€¢ MessageBubble: Individual message component              â”‚
â”‚  â€¢ TypingIndicator: Animated typing dots                    â”‚
â”‚  â€¢ API Client: Axios-based backend communication            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/REST API
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”
â”‚                         Backend                              â”‚
â”‚                    (FastAPI + Python)                        â”‚
â”‚                                                              â”‚
â”‚  Routes Layer:                                               â”‚
â”‚  â€¢ /api/messages - Send/receive messages                     â”‚
â”‚  â€¢ /api/users - User management                              â”‚
â”‚  â€¢ /api/typing - Typing indicator                            â”‚
â”‚                                                              â”‚
â”‚  Services Layer:                                             â”‚
â”‚  â€¢ LLMService: OpenRouter integration + context building     â”‚
â”‚  â€¢ MemoryService: Extract and retrieve long-term memories    â”‚
â”‚  â€¢ ProtocolService: Match and inject medical protocols       â”‚
â”‚  â€¢ CacheService: Redis caching for performance               â”‚
â”‚                                                              â”‚
â”‚  Data Layer:                                                 â”‚
â”‚  â€¢ Models: SQLAlchemy ORM models                             â”‚
â”‚  â€¢ Schemas: Pydantic request/response validation             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜
            â”‚                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€-â”´â”€â”€â”€â”€â”€â”
    â”‚   PostgreSQL   â”‚    â”‚    Redis    â”‚
    â”‚   (Database)   â”‚    â”‚   (Cache)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Tables:                       â”‚
    â”‚  â€¢ users                       â”‚
    â”‚  â€¢ messages                    â”‚
    â”‚  â€¢ memories                    â”‚
    â”‚  â€¢ protocols                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow

1. **User sends message** â†’ Frontend
2. **POST /api/messages** â†’ Backend API
3. **Store user message** â†’ PostgreSQL
4. **Set typing indicator** â†’ Redis
5. **Build LLM context**:
   - Fetch recent messages
   - Retrieve relevant memories
   - Match protocols
   - Manage token budget
6. **Call OpenRouter API** â†’ Generate AI response
7. **Store AI message** â†’ PostgreSQL
8. **Extract memories** (if interval reached) â†’ PostgreSQL
9. **Return response** â†’ Frontend
10. **Update UI** â†’ Display messages + clear typing indicator

---

## ğŸš€ Local Setup

### Prerequisites

Ensure you have the following installed:

- **Python 3.9+**
- **Node.js 18+**
- **PostgreSQL 12+**
- **Redis 6+**

### Step 1: Clone the Repository

```bash
git clone https://github.com/Saurabhdixit93/AI-Health-Coach
cd AI-Health-Coach
```

### Step 2: Database Setup

#### Install PostgreSQL (if not installed)

**macOS:**

```bash
brew install postgresql@15
brew services start postgresql@15
```

**Ubuntu/Debian:**

```bash
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo systemctl start postgresql
```

#### Create Database

```bash
# Access PostgreSQL
psql postgres

# Create database
CREATE DATABASE disha_db;

# Create user (optional)
CREATE USER disha_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE disha_db TO disha_user;

# Exit
\q
```

### Step 3: Redis Setup

#### Install Redis (if not installed)

**macOS:**

```bash
brew install redis
brew services start redis
```

**Ubuntu/Debian:**

```bash
sudo apt update
sudo apt install redis-server
sudo systemctl start redis-server
```

#### Verify Redis is running

```bash
redis-cli ping
# Should return: PONG
```

### Step 4: Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env

# Edit .env file and add your OpenRouter API key
# DATABASE_URL=postgresql://your-db-user:your-db-password@localhost:5432/disha_db
# REDIS_URL=redis://localhost:6379/0
# OPENROUTER_API_KEY=sk-or-v1-your-api-key
# AI_MODEL=openai/gpt-oss-120b:free

# Initialize database (creates tables and seeds protocols)
python -m app.init_db

# You'll see output like:
# âœ“ Tables created successfully
# âœ“ Created 6 new protocols
# âœ“ Created demo user with ID: <uuid>
# Save this user ID for testing!

# Start backend server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The backend will be available at: **http://localhost:8000**

API docs: **http://localhost:8000/docs**

### Step 5: Frontend Setup

Open a **new terminal window**:

```bash
cd frontend

# Install dependencies
npm install

# Create .env.local file
cp .env.local.example .env.local

# The default API URL should be: http://localhost:8000
# Edit if your backend runs on a different port

# Start development server
npm run dev
```

The frontend will be available at: **http://localhost:3000**

### Step 6: Test the Application

1. **Open browser**: Navigate to http://localhost:3000
2. **First visit**: Application creates a demo user automatically
3. **Start chatting**: Send "Hi!" to begin conversation with Disha
4. **Test features**:
   - Send health-related queries (fever, stomach ache, etc.)
   - Scroll up to test infinite scroll
   - Watch typing indicator while AI responds
   - Check if AI remembers context from earlier messages

---

## ğŸ’¡ How It Works

### 1. Context Management

The LLM service builds context from multiple sources:

```python
Context Components:
â”œâ”€â”€ System Prompt (~300 tokens)
â”‚   â””â”€â”€ Disha's persona, guidelines, and behavior
â”œâ”€â”€ Long-term Memories (~400 tokens)
â”‚   â””â”€â”€ Top 3-5 relevant memories about user
â”œâ”€â”€ Medical Protocols (~300 tokens)
â”‚   â””â”€â”€ Matched protocols based on keywords
â””â”€â”€ Recent Messages (~1500 tokens)
    â””â”€â”€ Last 10-15 messages (dynamically adjusted)

Total Input: ~2500 tokens (safe for 4K context window)
Max Output: 500 tokens
```

**Overflow Handling:**

- Monitor total token count before LLM call
- If > 3000 tokens, reduce message count
- Always keep system prompt + top 2 memories
- Prioritize recent conversation over old messages

### 2. Memory Extraction

Every 5 messages, the system extracts key information:

**Extraction Categories:**

- **Demographics**: Age, gender, location
- **Health Conditions**: Chronic illness, allergies
- **Medications**: Current treatments
- **Lifestyle**: Diet, exercise, sleep habits
- **Symptoms**: Recent concerns

**Storage:**

- Stored in `memories` table with importance score (0-1)
- Retrieved by relevance and recency
- Used to personalize future responses

### 3. Protocol Matching

**Keyword-based Matching:**

```javascript
User: "I have a fever and headache"
          â†“
Keywords detected: ["fever", "headache"]
          â†“
Matched protocols:
  - Fever Management
  - Headache Management
          â†“
Protocol instructions injected into LLM context
          â†“
AI responds with protocol-guided advice
```

**Seed Protocols:**

1. Fever Management
2. Stomach Issues
3. Cold and Cough
4. Headache Management
5. Emergency Situations (always advise seeking help)
6. Refund Policy (redirect to support)

### 4. Pagination Strategy

**Cursor-based Pagination** (not offset-based):

```
Advantages:
âœ“ Consistent results even with new data
âœ“ No duplicate messages
âœ“ Efficient for large datasets
âœ“ Better performance for infinite scroll

How it works:
1. Client requests: GET /api/messages?user_id=X&limit=50
2. Server returns: 50 messages + next_cursor (last message ID)
3. Client requests more: GET /api/messages?user_id=X&before=<cursor>&limit=50
4. Repeat until has_more = false
```

---

## ğŸ“š API Documentation

### Endpoints

#### **POST /api/users**

Create a new user.

**Request:**

```json
{
  "name": "John Doe",
  "metadata": {
    "age": 30,
    "location": "Mumbai"
  }
}
```

**Response:**

```json
{
  "id": "uuid",
  "name": "John Doe",
  "metadata": {...},
  "created_at": "2025-12-23T...",
  "updated_at": "2025-12-23T..."
}
```

---

#### **GET /api/users/{user_id}**

Get user by ID.

---

#### **POST /api/messages**

Send a message and receive AI response.

**Request:**

```json
{
  "user_id": "uuid",
  "content": "I have a fever",
  "is_onboarding": false
}
```

**Response:**

```json
{
  "user_message": {
    "id": "uuid",
    "role": "user",
    "content": "I have a fever",
    "created_at": "..."
  },
  "ai_response": {
    "id": "uuid",
    "role": "assistant",
    "content": "I'm sorry to hear that...",
    "created_at": "..."
  }
}
```

---

#### **GET /api/messages**

Get paginated message history.

**Query Parameters:**

- `user_id` (required): User UUID
- `before` (optional): Cursor for pagination
- `limit` (optional): Number of messages (default: 50, max: 100)

**Response:**

```json
{
  "messages": [...],
  "has_more": true,
  "next_cursor": "uuid"
}
```

---

#### **GET /api/typing/{user_id}**

Get typing indicator status.

**Response:**

```json
{
  "is_typing": true,
  "user_id": "uuid"
}
```

---

#### **GET /api/health**

Health check endpoint.

---

## ğŸ¤– LLM Integration

### Provider & Configuration

**Provider**: [OpenRouter](https://openrouter.ai)  
**Model**: `openai/gpt-4o-mini` (fallback: `openai/gpt-oss-120b:free`)  
**Temperature**: 0.7 (balanced creativity and consistency)  
**Max Output Tokens**: 500 (concise, mobile-friendly responses)  
**Max Input Tokens**: ~2500 (safe for 4K context window)

### Why OpenRouter?

1. **Cost-effective**: Free tier available with multiple models, pay-as-you-go pricing
2. **Flexibility**: Easy to switch between providers (OpenAI, Anthropic, Google, etc.)
3. **No vendor lock-in**: OpenAI-compatible API works with existing code
4. **Reliability**: High uptime, good performance, automatic failover
5. **Developer-friendly**: Simple integration, good documentation

### System Prompt Design

The AI persona is carefully crafted to act as **"Disha"** - a warm, empathetic health coach:

```python
SYSTEM_PROMPT = """
You are Disha, a caring and knowledgeable AI health coach for Indian users.

Personality:
- Warm and empathetic (like a caring friend on WhatsApp)
- Professional but conversational
- Culturally aware (Indian context, dietary habits, climate)
- Encouraging and supportive

Guidelines:
- Keep responses SHORT (2-4 sentences max for mobile)
- Ask clarifying questions to understand better
- Provide actionable, simple advice
- Use everyday language, avoid medical jargon
- Be mindful of Indian context (monsoons, festivals, local foods)

CRITICAL Safety Rules:
- NEVER diagnose medical conditions
- NEVER prescribe medications
- ALWAYS recommend seeing a doctor for serious symptoms
- For emergencies (chest pain, difficulty breathing, severe injuries):
  "ğŸš¨ This sounds serious. Please visit a doctor or call emergency services immediately."

Response Style:
- Use emojis sparingly (1-2 per message)
- Be concise - users are on mobile
- Personalize based on their history
- Show you remember previous conversations
"""
```

### How We're Prompting the LLM

#### 1. **Layered Context Building**

We build context from multiple sources in priority order:

```python
Context Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. System Prompt (~300 tokens)             â”‚  â† Always included
â”‚    â””â”€â”€ Disha's personality & safety rules   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Relevant Memories (~400 tokens)         â”‚  â† Top 3-5 by relevance
â”‚    â””â”€â”€ User: "32yo, diabetic, exercises"    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Matched Protocols (~300 tokens)         â”‚  â† Keyword-based matching
â”‚    â””â”€â”€ "Fever Management: advise rest..."   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Recent Messages (~1500 tokens)          â”‚  â† Last 10-15 messages
â”‚    â””â”€â”€ User â†” AI conversation history       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: ~2500 tokens (safe margin for 4K limit)
```

#### 2. **Dynamic Token Management**

```python
# Simplified version of our token management
def build_context(user_id, user_message):
    # Start with fixed components
    context = [system_prompt]  # ~300 tokens

    # Add relevant memories (importance-weighted)
    memories = get_top_memories(user_id, limit=5)
    context.append(format_memories(memories))  # ~400 tokens

    # Add matched protocols (keyword-based)
    protocols = match_protocols(user_message)
    context.append(format_protocols(protocols))  # ~300 tokens

    # Add recent messages (dynamically adjusted)
    messages = get_recent_messages(user_id, limit=15)

    # Token budget check
    current_tokens = estimate_tokens(context)
    remaining_budget = 2500 - current_tokens

    # Fit as many messages as possible
    fitted_messages = fit_messages_to_budget(
        messages,
        token_budget=remaining_budget
    )
    context.append(fitted_messages)

    return context
```

**Token Estimation**: We use character-based estimation (`len(text) // 4`) as a lightweight alternative to `tiktoken` for deployment compatibility.

#### 3. **Memory-Aware Prompting**

When relevant memories exist, we inject them as context:

```
User Memory Context:
---
From your previous conversations, I know:
â€¢ You're a 32-year-old software engineer in Bangalore
â€¢ You have type 2 diabetes (on metformin)
â€¢ You exercise 3x per week
â€¢ You're vegetarian and prefer south Indian food
â€¢ You've been working on improving sleep quality
---

[Continue normal conversation with this context]
```

#### 4. **Protocol-Guided Responses**

When keywords match protocols, we inject specific instructions:

```
Medical Protocol: Fever Management
---
For fever queries:
1. Ask about temperature, duration, other symptoms
2. Suggest: rest, hydration (2-3L water), paracetamol if >100Â°F
3. Recommend doctor if: >102Â°F for 3+ days, difficulty breathing
4. Advise against: aspirin for children, cold showers
---

User: "I have a fever"
AI: [Follows protocol while maintaining Disha's warm tone]
```

### Prompt Engineering Techniques Used

1. **Few-shot prompting**: System prompt includes example responses
2. **Constraint-based**: Explicit length limits ("2-4 sentences")
3. **Safety guardrails**: Multiple layers of "never diagnose" rules
4. **Context injection**: Memories + protocols without explicit mention
5. **Persona consistency**: Maintains "Disha" character across all responses

### Handling Edge Cases

- **Token overflow**: Reduce message history, keep memories + protocols
- **No memories yet**: Gracefully skip memory section
- **No protocol match**: General health advice mode
- **Emergency keywords**: Hardcoded responses bypass normal flow
- **API failures**: Retry logic with exponential backoff (3 attempts)

### Cost Optimization

- **Caching**: Protocols cached in Redis (24h TTL)
- **Batching**: Memory extraction every 5 messages (not every message)
- **Model selection**: Free tier models first, paid models as fallback
- **Output limits**: Max 500 tokens keeps costs low
- **Streaming**: Future improvement for better UX without extra cost

---

## ğŸ”„ Trade-offs & Design Decisions

### Architectural Decisions

| Decision                                          | Why This Approach                                                                                             | Trade-off                                                                           | Alternative Considered                                                  |
| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **FastAPI over Flask**                            | Native async/await support, auto-generated OpenAPI docs, modern type hints, better performance                | Smaller ecosystem than Flask, newer framework (less Stack Overflow answers)         | Flask with async extensions, Django                                     |
| **Cursor pagination**                             | Consistent results with concurrent inserts, better performance for large datasets, no duplicate/missing items | More complex implementation than offset, requires indexed timestamps                | Offset-based pagination, keyset pagination                              |
| **Keyword matching for protocols**                | Zero latency, no external API calls, works offline, simple to debug                                           | Less accurate than semantic search, can miss synonyms/variations                    | Embeddings + vector search (Pinecone), NER models                       |
| **Periodic memory extraction (every 5 messages)** | Balances cost vs. context quality, reduces API calls by 80%                                                   | May miss important info said between intervals                                      | Real-time extraction (expensive), user-triggered save                   |
| **Single session model**                          | Simpler UX (no create/delete confusion), easier backend state management                                      | Can't have separate conversations for different topics, single history can get long | Multi-session with conversation management                              |
| **Redis for caching**                             | Sub-millisecond latency, widely adopted, simple key-value model                                               | Additional service to manage, cache invalidation complexity                         | Memcached (simpler), in-memory dict (simpler but not distributed)       |
| **SQLAlchemy ORM**                                | Type safety, automatic migrations (Alembic), relationship management                                          | Some performance overhead vs. raw SQL, learning curve                               | Raw SQL (faster), Peewee (lighter), Django ORM                          |
| **WhatsApp-like UI**                              | Users already familiar, mobile-first, conversational feel                                                     | May need desktop optimizations, less formal than business chat                      | ChatGPT-like interface, Slack-style                                     |
| **Character-based token counting**                | No external dependencies, works everywhere, instant                                                           | ~15% accuracy vs. tiktoken, can underestimate tokens                                | tiktoken (accurate but Rust compilation required), LLM API token counts |
| **PostgreSQL over NoSQL**                         | ACID transactions, mature ecosystem, complex queries, joins                                                   | Vertical scaling limits (eventually), schema migrations                             | MongoDB (flexible schema), DynamoDB (serverless)                        |

### What Went Well âœ…

1. **Deployment Compatibility**: Removed tiktoken, upgraded dependencies â†’ now deploys on Render free tier
2. **Context Management**: Layered approach keeps responses relevant without token overflow
3. **User Experience**: WhatsApp-like interface feels familiar and approachable
4. **Type Safety**: Pydantic + TypeScript catch errors at dev time, not runtime
5. **Cursor Pagination**: Infinite scroll works smoothly even with 1000+ messages

### Known Limitations âš ï¸

1. **No Real-time Updates**: Frontend polls for typing indicator instead of WebSockets
2. **Single User Per Session**: Can't switch users without page reload
3. **Memory Extraction Gaps**: Important info between 5-message intervals may be missed
4. **Protocol Matching Accuracy**: Keyword-based matching misses synonyms (e.g., "temperature" vs "fever")
5. **No Authentication**: Anyone with API URL can send messages
6. **Token Estimation**: Character-based counting is ~15% less accurate than tiktoken
7. **No Message Editing**: Can't edit or delete sent messages
8. **Basic Error Handling**: Some edge cases show generic "Something went wrong"

### Technical Debt ğŸ“Š

1. **No Database Migrations**: Using `create_all()` instead of Alembic migrations
2. **Hardcoded System Prompt**: Should be configurable per user type (parent, athlete, senior)
3. **No Rate Limiting**: Vulnerable to spam/abuse
4. **Manual Testing**: No automated integration tests for API endpoints
5. **No Monitoring**: No observability into LLM costs, latency, or errors
6. **Env Var Management**: Some config in code, should all be in environment variables

### Security Considerations ğŸ”’

âš ï¸ **This is a demo/assignment project**. For production deployment:

- [ ] **Authentication**: Implement JWT-based auth with user registration
- [ ] **Authorization**: Role-based access control (RBAC)
- [ ] **Rate Limiting**: Prevent API abuse (e.g., 100 req/min per user)
- [ ] **Input Sanitization**: Prevent SQL injection, XSS attacks
- [ ] **HTTPS/TLS**: Encrypt data in transit
- [ ] **API Key Rotation**: Rotate OpenRouter key periodically
- [ ] **CORS Restrictions**: Lock down to specific frontend domains
- [ ] **Logging**: Track user actions, API calls for audit
- [ ] **Data Privacy**: GDPR/HIPAA compliance (if handling health data)
- [ ] **Content Filtering**: Prevent malicious prompts, jailbreaks

### Performance Optimizations Applied

- âœ… **Redis Caching**: Protocols cached for 24h (reduces DB queries by 90%)
- âœ… **Connection Pooling**: PostgreSQL connection pool (5-20 connections)
- âœ… **Cursor Pagination**: O(1) lookups with indexed timestamps
- âœ… **Lazy Loading**: Messages loaded on-demand (not all at once)
- âœ… **Token Counting Pre-check**: Prevents failed LLM calls due to overflow
- âœ… **Async Endpoints**: FastAPI async handlers for I/O operations

---

## ğŸš§ Future Improvements

### If I Had More Time...

#### High Priority

- **WebSockets**: Real-time bidirectional communication instead of polling
- **Vector Database**: Use Pinecone/Weaviate for semantic memory search
- **Message Streaming**: Stream AI responses token-by-token for better UX
- **Authentication**: JWT-based auth with signup/login
- **Multi-session Support**: Allow users to create/delete conversations

#### Medium Priority

- **Advanced RAG**: Embed medical knowledge base for better protocol matching
- **Voice Input/Output**: WhatsApp-style voice messages
- **File Uploads**: Share medical reports, prescriptions
- **Better Onboarding**: Adaptive flow based on user type
- **Analytics Dashboard**: Track engagement, common queries

#### Nice to Have

- **Sentiment Analysis**: Detect user mood and adjust tone
- **Wearables Integration**: Sync with Fitbit, Apple Health
- **Multilingual Support**: Hindi, Tamil, Bengali, etc.
- **Export Chat**: Download conversation as PDF
- **A/B Testing**: Experiment with different prompts
- **Mobile Apps**: Native iOS/Android apps

---

## ğŸ› Troubleshooting

### Backend Issues

**Port already in use:**

```bash
# Find process using port 8000
lsof -ti:8000 | xargs kill -9

# Or use a different port
uvicorn app.main:app --reload --port 8001
```

**Database connection error:**

```bash
# Check if PostgreSQL is running
pg_isready

# Check database exists
psql -l | grep disha_db
```

**Redis connection error:**

```bash
# Check if Redis is running
redis-cli ping

# Start Redis
brew services start redis  # macOS
sudo systemctl start redis  # Linux
```

### Frontend Issues

**API connection failed:**

- Verify backend is running on port 8000
- Check `.env.local` has correct `NEXT_PUBLIC_API_URL`
- Check CORS settings in backend config

**Node modules error:**

```bash
rm -rf node_modules package-lock.json
npm install
```

---

## ğŸ“ License

This project is created for educational/assignment purposes.

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ for Curelink (Disha AI Health Coach Assignment)

---

## ğŸ“ Support

For issues or questions:

1. Check the troubleshooting section above
2. Review API docs at http://localhost:8000/docs
3. Check logs in terminal for error messages

---

**Enjoy chatting with Disha! ğŸ¥ğŸ’š**
